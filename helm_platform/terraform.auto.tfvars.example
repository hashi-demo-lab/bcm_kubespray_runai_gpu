# Helm Platform Configuration Example
# Copy to terraform.auto.tfvars and fill in sensitive values
# Feature: Run:AI Platform Deployment on BCM Kubernetes
#
# Usage:
#   cp terraform.auto.tfvars.example terraform.auto.tfvars
#   # Edit terraform.auto.tfvars with your values
#   terraform apply

# =============================================================================
# Kubernetes Cluster Connection
# =============================================================================
# Uses kubeconfig file by default (recommended)
# The file is generated by the root terraform module after cluster deployment

kubeconfig_path = "../kubeconfig"

# Alternative: direct API connection (leave empty to use kubeconfig)
# kubernetes_host              = "https://10.184.162.102:6443"
# kubernetes_ca_certificate    = ""  # Base64-encoded CA cert
# kubernetes_client_certificate = ""  # Base64-encoded client cert
# kubernetes_client_key        = ""  # Base64-encoded client key

# =============================================================================
# Cluster Metadata
# =============================================================================

cluster_name     = "bcm-k8s-cluster"
control_plane_ip = "10.184.162.102"  # First control plane node
worker_ips       = ["10.184.162.109", "10.184.162.110"]  # DGX GPU nodes

# =============================================================================
# Run:AI Configuration (Self-Hosted v2.21)
# Docs: https://run-ai-docs.nvidia.com/self-hosted/2.21/
# =============================================================================

enable_runai = true

# --- JFrog Container Registry Credentials ---
# Required for pulling Run:AI container images
# Obtain from NVIDIA support or your Run:AI license agreement

runai_jfrog_username = "self-hosted-image-puller-prod"
runai_jfrog_token    = ""  # REQUIRED: JFrog token from NVIDIA

# --- Helm Chart Credentials (NOT NEEDED - repos are public) ---
# The cp-charts-prod and run-ai-charts repositories are publicly accessible
# These variables are kept for backwards compatibility but can be left empty

runai_helm_username = ""
runai_helm_token    = ""

# --- Control Plane (Backend) Configuration ---

runai_backend_version = "2.21.63"
runai_domain          = "bcm-head-01.eth.cluster"  # FQDN accessible to users
runai_admin_email     = "admin@example.com"
runai_admin_password  = ""  # REQUIRED: Min 8 chars, 1 digit, 1 lowercase, 1 uppercase, 1 special

# --- Cluster Component (Phase 2) ---
# These values are obtained from the Run:AI UI AFTER deploying the control plane
# 1. Deploy control plane (terraform apply with above values)
# 2. Login to https://<runai_domain>
# 3. Create a new cluster in the UI
# 4. Copy client_secret and cluster_uid from the UI
# 5. Update these values and run terraform apply again

runai_cluster_version = "2.21"
runai_client_secret   = ""  # From UI: Settings > Clusters > New Cluster
runai_cluster_uid     = ""  # From UI: Settings > Clusters > New Cluster

# =============================================================================
# TLS Configuration
# =============================================================================

generate_self_signed_cert = true  # Set false to provide your own certs

# Provide your own certs (only if generate_self_signed_cert = false)
# runai_tls_cert = ""  # PEM format certificate
# runai_tls_key  = ""  # PEM format private key

# =============================================================================
# NVIDIA GPU Operator
# =============================================================================

enable_gpu_operator  = true
gpu_operator_version = "v25.3.3"

# Driver configuration - set false if drivers pre-installed on DGX nodes
gpu_driver_enabled = true
gpu_driver_version = "550.54.15"

# Container toolkit - set false if pre-installed on DGX nodes
gpu_toolkit_enabled = true

# =============================================================================
# Supporting Components
# =============================================================================

# --- Ingress Controller ---
enable_ingress_nginx  = true
ingress_nginx_version = "4.9.0"

# --- Storage ---
enable_local_storage  = true
local_storage_version = "0.0.26"

# --- Prometheus Stack (metrics) ---
enable_prometheus        = true
prometheus_stack_version = "77.6.2"
enable_grafana           = true
grafana_admin_password   = "admin"  # Change in production

# --- Prometheus Adapter (custom metrics for Run:AI) ---
enable_prometheus_adapter  = true
prometheus_adapter_version = "5.1.0"

# --- Metrics Server (kubectl top, HPA) ---
enable_metrics_server  = true
metrics_server_version = "3.13.0"

# --- LeaderWorkerSet Operator (distributed training) ---
enable_lws_operator  = true
lws_operator_version = "v0.7.0"

# --- Knative Operator (serverless inference) ---
enable_knative_operator  = true
knative_operator_version = "v1.16.0"
enable_knative_serving   = false  # Enable after operator is deployed
